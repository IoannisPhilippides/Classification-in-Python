{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Nearest Neighbours document classification\n",
    "\n",
    "In this exercise, you will write a program that can classify documents into any number of classes, based on provided training data. There are many classification algorithms; we will look at one of the simplest possible methods, that still often turns out to work quite well in practice: the *nearest neighbour classifier*.\n",
    "\n",
    "The idea is as follows:\n",
    "1. We define a distance function between documents. The distance between two documents that are equal is zero. The more different the documents are, the larger the distance.\n",
    "2. Given a new document that we want to classify, we calculate the distance between it and *all* documents in the training data. One of them will be closest according to the distance measure we defined above. We report the class of the closest match as the predicted class for the new document.\n",
    "\n",
    "You can test your program on the spam dataset that is provided via blackboard. We provide lots of hints as to how to process your data and how to set up your distance function; but some parts of the design will be yours to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "\n",
    "We provided some code that allows you to read in an email document. The first step is to be able to figure out all the files that are in some directory.\n",
    "\n",
    "Try out the filelist function below to read and print the files in one of the email directories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir           # used to see what files are in a directory\n",
    "from os.path import isfile, join # join concatenates parts of a pathname\n",
    "\n",
    "# in:  the pathname of a directory\n",
    "# out: the list of files in that directory\n",
    "def filelist(path):\n",
    "    return [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filelist(\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/spam/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read all the contents of a file into a Python string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in:  the pathname of a directory, and the name of a file in that directory\n",
    "# out: a string containing the contents of the file\n",
    "def read_file(path, name):\n",
    "    handle = open(join(path, name), \"rb\")#rb means that it will treat emails as binary\n",
    "    res = handle.read()\n",
    "    handle.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_file=read_file(\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/test/spam\",'0006.7a32642f8c22bbeb85d6c3b5f3890a2c' )\n",
    "string_file = str(string_file)\n",
    "#print(string_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "To simplify our task, we are going to ignore the word order and word frequencies in the email messages. To do so, modify the function below so it converts each document string into a *set of words*. Include only words that consist solely of alphabetic characters, and convert all words to lower case.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  a long string containing a document\n",
    "# out: a (python) set containing all words in the document, in lower case.\n",
    "def extract_words(file):\n",
    "    import re\n",
    "    new_file = re.sub(\"[^a-zA-Z ]+\", \"\", file)\n",
    "    new_file_lower=str.lower(new_file)\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  new_file_lower).split()\n",
    "    return(list(set(wordList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract_words(string_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Now that we can read word sets, it is time to decide on an appropriate distance function. We leave the definition of this function up to you: adapt the skeleton code below into a function that you believe will work well, and briefly motivate your choice (in perhaps two lines of text).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  two word sets\n",
    "# out: their distance (0 if the sets are equal)\n",
    "#think about normalization of distance for example divinding by measure of email \n",
    "#short emails will always have shortest distance\n",
    "     \n",
    "def distance(wordset1, wordset2):\n",
    "    common_list=[element for element in wordset1 if element in wordset2]\n",
    "    distance=(len(wordset1)-len(common_list))/len(wordset1)\n",
    "    return distance\n",
    "#     if wordset1 in string_file and wordset2 in string_file:\n",
    "#        return abs(words.index(wordset1) - words.index(wordset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(\"only\", \"remove\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Motivation: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "We can now compare individual documents. However, we do not have any code for working with all documents in a class simultaneously. A class will be represented as a *dictionary*, mapping the name of each file it contains to the set of words in that file.\n",
    "\n",
    "\n",
    "Adapt the read_class function below to create the dictionary given the name of a directory containing the files for that class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  the pathname of a directory containing all files that describe a class\n",
    "# out: a dictionary mapping each filename to its word set\n",
    "#call it twice\n",
    "def read_class(path):\n",
    " file_list=filelist(path)\n",
    " import re \n",
    " my_dictionary={}\n",
    " for file in file_list:\n",
    "    string_file=str(read_file(path,file))\n",
    "    my_dictionary[file]=extract_words(string_file)\n",
    " return my_dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/train/spam\"\n",
    "#read_class(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Now that we can obtain the representation of a training class, we need to be able to find how well a test document matches against that class.\n",
    "\n",
    "To do so, the function below should look for the *best matching document* in the class, and return its name and its distance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  a wordset and a class\n",
    "# out: the tuple (f,d) where f is the filename of the best match, and\n",
    "#                            d is its distance to the given wordset.\n",
    "#class means probably a file\n",
    "#one possible way..attach to dictionary another information about distance and then \n",
    "#take minimum of distance...name of false should be also in dictionary\n",
    "def best_match(wordset, cls):\n",
    "    class_files=filelist(path)\n",
    "    cls=read_class(path)\n",
    "    dist={}\n",
    "    for file in class_files:\n",
    "     dist[file]=distance(wordset,cls[file]) \n",
    "    minimum=min(dist.items(), key=lambda x: x[1]) \n",
    "    return(minimum)#gives name of fale and distance to nearest neighboor in that class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0446.d971f234fa00ed94e18f5ce8f5c0f852', 0.3333333333333333)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordset=[\"yes\",\"and\", \"something\"]\n",
    "best_match(wordset,path)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "This is where it gets a little bit abstract. Just like we have created classes, that are dictionaries which map filenames to wordsets, it will be convenient to bunch up all training classes into a single dictionary, that maps the name of the class to the data for that class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  the names of all the considered classes\n",
    "# out: a dictionary mapping each class name to its data\n",
    "#class names spam and ham?\n",
    "def read_training_data(class_names):\n",
    "    class_dict={}\n",
    "    path=\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/train\"\n",
    "    s=\"/\" \n",
    "    for c in class_names:\n",
    "     seq=(path,c)\n",
    "     path1=s.join(seq)\n",
    "     class_dict[c]=read_class(path1)\n",
    "    return(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names=[\"spam\",\"ham\"]\n",
    "#read_training_data(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "It is now finally time to implement the nearest neighbour algorithm. The function below should find the best match for the given wordset in *each* class of the training data, and return which file matched best, at what distance that file matched, and what class it belongs to. That class will be the predicted class for the given wordset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in:  a wordset and a dict of classes\n",
    "# out: the filename of the closest file, the distance to that file, and the name of its class\n",
    "def nearest_neighbour(wordset, training_data):\n",
    "    dis={}\n",
    "    for clas in training_data.keys():\n",
    "        dis[clas]=best_match(wordset,training_data[clas]) \n",
    "    minimum=min(dis.items(), key=lambda x: x[1]) \n",
    "    return(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ham', ('0446.d971f234fa00ed94e18f5ce8f5c0f852', 0.3333333333333333))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=read_training_data(class_names)\n",
    "nearest_neighbour(wordset,training_data)\n",
    "\n",
    "#training_data[\"spam\"][\"0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "Write a test function that will run through all the files in a test directory and predict their class based on the nearest neighbour algorithm using the training data. Report which files get misclassified and at the end report what fraction of files are classified correctly.\n",
    "\n",
    "Also write a main function that applies the test to all test directories. Make sure it works when we run it!\n",
    "\n",
    "The sensitivity of the classifier is the fraction of test spam mails that are correctly classified. The specificity is the fraction of test ham mails that are correctly classified. Do these scores turn out similar for you? Why or why not?\n",
    "\n",
    "The best scores for sensitivity and specificity will be reported in class :-)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-80-f6f10aaf27fa>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-f6f10aaf27fa>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    ratio=\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# in:  a dict of training classes, and the name of the class to test\n",
    "# out: a report of the test success\n",
    "def test(training_data, test_class_name):\n",
    "    print(\"Testing \", test_class_name)\n",
    "    path=\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/test\"\n",
    "    s=\"/\"\n",
    "    path1=s.join(seq(\"path\",test_class_name))\n",
    "    clas_test=read_class(path1)\n",
    "    dic={}\n",
    "    for file in clas_test:\n",
    "       dic[file]=nearest_neighbour(file,training_data)[0]#assigns class to file\n",
    "def main():\n",
    "    None # Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test(training_data,\"spam\")\n",
    "#test_class_name=\"spam\"\n",
    "#path11=\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/test\"\n",
    "#s=\"/\"\n",
    "#seq=(path,test_class_name)\n",
    "pathh=\"C:/Users/ntb/Desktop/LEIDEN Ucebne Materialy/Introduction to data science/week3Python/data/test/spam\"\n",
    "clas_test1=read_class(pathh)\n",
    "dic={}\n",
    "#for file in clas_test.keys():\n",
    "#    dic[file]=nearest_neighbour(clas_test[file],training_data)[0]#assigns class to file\n",
    "#print(dic)\n",
    "\n",
    "print(clas_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
